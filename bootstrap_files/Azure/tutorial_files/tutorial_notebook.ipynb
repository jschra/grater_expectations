{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grater Expectations Tutorial\n",
    "\n",
    "![Grater Expectations](https://raw.githubusercontent.com/jschra/grater_expectations/dev/docs/images/grater_expectations_background_small.png)\n",
    "\n",
    "### Introduction\n",
    "Welcome to the Grater Expectations tutorial! This notebook will help you run through a full example of using Grater Expectations. To repeat on what is already mentioned in the README, note that you need the following to run all the components of the tutorial:\n",
    "\n",
    "-  Azure account with [Service Principal access keys](https://learn.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals)\n",
    "- [Docker Engine](https://docs.docker.com/engine/): to create new images to run on AWS Lambda and push them to ACR\n",
    "- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli): to login to Azure, create an ACR repository and push docker images to ACR\n",
    "- Python 3.8: It is recommended to use conda ([Miniconda](https://docs.conda.io/en/latest/miniconda.html)) for easy environment creation and management\n",
    "- [Terraform](https://www.terraform.io/): to spin up a storage account and containers for GE artifacts and the Data Docs website and an Azure function for testing\n",
    "- IDE (e.g. VS Code, optional): for easier development (not necessarily for notebooks, but definitely for Python files)\n",
    "\n",
    "If you have these installed, then you are ready to continue with the tutorial!\n",
    "\n",
    "<hr>\n",
    "\n",
    "In order to validate your data, Great Expectations is a package that offers a battery-included set of logic to get up-and-running fast. Fully figuring out how Great Expectations works and applying it to your project, however, can be somewhat involved. This is what Grater Expectations and this tutorial help you with!\n",
    "\n",
    "Grater Expectations makes a few choices for you and offers scripts, configurations and notebooks to get you started. The choices that were made are:\n",
    "\n",
    "- Great Expectations output will be stored on Azure Blob storage\n",
    "- The rendered Data Docs site will be stored on Azure Blob storage (hosted as static website)\n",
    "- You will write your own data loading logic to read data into memory as a pandas DataFrame\n",
    "- You will write your own set of expectations to test the quality of this data\n",
    "- The validation logic will be deployed as Docker container via an Azure function\n",
    "\n",
    "To set you up for the above, you already entered configurations in `testing_config.yml` for the tutorial, which will be used throughout the code to generate AWS services and access them.\n",
    "\n",
    "<hr>\n",
    "\n",
    "To get an idea of how Grater Expectations can help you to develop and deploy logic that you can use to test data, a simplified workflow is shown below. \n",
    "\n",
    "![Workflow](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/high_level_workflow_azure.png)\n",
    "\n",
    "The overall idea is to **configure** data testing by using Great Expectations, so that a preset selection of tests can be run over new data at **runtime**.\n",
    "\n",
    "In order to **configure** Great Expectations, an example dataset representative of future data to be tested is loaded. Using this dataset, multiple tests (*expectations*) are defined and bundled into a set of tests (*expectation suite*). In order to call this expectation suite at runtime, it is connected to a checkpoint. This checkpoint can then be called at runtime to test new data against, the logic of which is developed aw Python script and built into a Docker image. This image is deployed as an Azure function.\n",
    "\n",
    "At **runtime**, a POST request containing information on which new data to load and test can then be sent to the deployed Function to invoke it. The Function will then load and validate the new data, using the checkpoint and expectation suite previously developed. The results will then be published on a so-called Data Docs website, which end users can then inspect.\n",
    "\n",
    "In the rest of this tutorial, each of the steps to set up services, configure Great Expectations and validate new data will be detailed.\n",
    "\n",
    "### Table of contents\n",
    "This notebook will guide you through each of the steps to get you testing your data as soon as possible. The steps are:\n",
    "\n",
    "1. [Setting up a virtual environment](#setting-up-a-virtual-environment)\n",
    "2. [Provisioning storage](#provisioning-storage)\n",
    "3. [Imports and configurations](#imports-and-configurations)\n",
    "4. [Initialization of objects](#initialization-of-objects)\n",
    "5. [Uploading tutorial data to Blob Storage](#uploading-tutorial-data-to-blob-storage)\n",
    "6. [Loading data](#loading-data)\n",
    "7. [Generating a batch request, expectation suite and validator](#generating-a-batch-request-expectation-suite-and-validator)\n",
    "8. [Creating Expectations](#writing-expectations)\n",
    "9. [Finalizing expectation suite and creating checkpoint](#finalizing-expectation-suite-and-creating-checkpoint)\n",
    "10. [Instantiating the Data Docs website](#instantiating-the-data-docs-website)\n",
    "11. [Developing logic for the Azure Function](#developing-logic-for-the-azure-function)\n",
    "12. [Creating a Docker image and deployment on Azure ACR](#creating-a-docker-image-and-deployment-on-azure-acr)\n",
    "13. [Deploying the Azure Function](#deploying-the-azure-function)\n",
    "14. [Validating new batches of data](#validating-new-batches-of-data)\n",
    "15. [Wrap up of tutorial and clean-up of Azure services](#wrap-up-of-tutorial-and-clean-up-of-azure-services)\n",
    "16. [End of tutorial](#end-of-tutorial)\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Setting up a virtual environment\n",
    "In order to run the logic contained within this notebook, make sure that it was started up from a virtual environment that contained all required Python dependencies. The easiest way to assure this is to first make a virtual environment for the project and then install `grater_expectations` within it.\n",
    "\n",
    "To create a new virtual environment, e.g. for python 3.8, and installing the package and its dependencies, run the following:\n",
    "\n",
    "<br>\n",
    "\n",
    "**Option 1: Pip**\n",
    "\n",
    "```bash\n",
    "# Create a virtual environment\n",
    "python -m venv env\n",
    "\n",
    "# Activate the virtual environment\n",
    "env/Scripts/Activate # Windows\n",
    "source env/bin/activate # MacOS\n",
    "\n",
    "# Install into the virtual envirpnment\n",
    "pip install grater_expectations\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Option 2 - Anaconda**\n",
    "\n",
    "```bash\n",
    "# Create a conda environment\n",
    "conda create --name grater_expectations python=3.8\n",
    "\n",
    "# Activate the conda environment\n",
    "conda activate grater_expectations\n",
    "\n",
    "# Install into the virtual environment\n",
    "conda install grater_expectations\n",
    "```\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provisioning storage\n",
    "For testing, Grater Expectations is configured to interact with containers in a storage account on AWS. To be able to run all the code in this notebook the **storage_account** and **data_container_name** as configured in the testing_config.yml must be provisioned, along with the **resource_group_name** that will be provisioned to deploy services in.\n",
    "\n",
    "To do so, auto-generated Terraform files can be found in the *terraform/storage* directory of this project. To use these configurations to generate storage in that directory, open a (Git bash) terminal (if you didn't already do so), set your Azure programmatic access credentials as environment variables and run the following commands:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "# Set credentials\n",
    "export ARM_CLIENT_ID=\"<enter_client_id_here>\"\n",
    "export ARM_CLIENT_SECRET=\"<enter_client_secret_here>\"\n",
    "export ARM_SUBSCRIPTION_ID=\"<enter_subscription_id_here>\"\n",
    "export ARM_TENANT_ID=\"<enter_tenant_id_here>\"\n",
    "\n",
    "# Go to correct Terraform directory\n",
    "cd terraform/storage\n",
    "\n",
    "# Initialize Terraform\n",
    "terraform init\n",
    "\n",
    "# Generate deployment plan, enter yes at the prompt if the plan is correct\n",
    "terraform apply\n",
    "```\n",
    "<br>\n",
    "\n",
    "Terraform will then provide you with a plan, which it will deploy if you enter 'yes' at the prompt as shown below.\n",
    "\n",
    "![Terraform bucket prompt](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/tutorial_tf_storage_prompt.png)\n",
    "\n",
    "After entering yes, Terraform will deploy the buckets and will show you the following output when finished.\n",
    "\n",
    "![Terraform bucket apply](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/tutorial_tf_storage_apply.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "You can check if the storage was successfully created by logging in to the portal and following [this link](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Storage%2FStorageAccounts)\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and configurations\n",
    "In the cell below, packages are imported and configurations are loaded from the `project_config.yml`, which was automatically created by `initialize_project.py`. This config file is based on the parameters set in `testing_config.yml` at the root of this repository\n",
    "\n",
    "Note specifically the import of load_csv_from_container as load_data. For this tutorial, a function was already developed to load csv files from a Blob Storage and turn these into pandas DataFrames. You can inspect the code for this function in `supporting_functions.py` or by calling `help(load_data)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports of Python and Grater Expectations packages and logic\n",
    "import great_expectations as ge\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.storage import StorageManagementClient \n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "from supporting_functions import (TestingConfiguration,\n",
    "                                  copy_tf_env_vars_for_az,\n",
    "                                  get_connection_string,\n",
    "                                  add_connection_string_to_config,\n",
    "                                  print_ge_site_link,\n",
    "                                  generate_link_in_notebook,\n",
    "                                  get_file_keys_from_container)\n",
    "from supporting_functions import load_csv_from_container as load_data\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "# -- Copy environment variables to Azure equivalents for python SDK\n",
    "copy_tf_env_vars_for_az()\n",
    "\n",
    "# -- Load parameters from configuration file\n",
    "test_config = TestingConfiguration(\"project_config.yml\")\n",
    "test_config.load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to access data in the storage account, below credentials are retrieved and an object is initialized. Using this object, a connection string is retrieved for the storage account, so that it can be used in the Great Expectations configuration file to establish connections.\n",
    "\n",
    "After running the cell below, you can evaluate this by checking out the `great_expectations/great_expectations.yml` file and check if it now contains connection strings throughout the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Obtain Azure credentials, initialize storage client\n",
    "credentials = DefaultAzureCredential()\n",
    "storage_client = StorageManagementClient(credentials, os.environ.get(\"AZURE_SUBSCRIPTION_ID\"))\n",
    "\n",
    "# -- Add connection string of storage account to GE config\n",
    "connection_string = get_connection_string(storage_client, test_config)\n",
    "add_connection_string_to_config(connection_string, test_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of objects\n",
    "Next a Great Expectations DataContext is initialized.\n",
    "\n",
    "By loading the GE configuration file (`great_expectations.yml` found in the great_expectations subdirectory of this project. The DataContext class defaults to this location and file when called), the `DataContext` object stores important parameters for you to interact with GE and automatically interact with S3 buckets for storing output, pulling checkpoints and generating Data Docs sites. For more information on data contexts, check out [the documentation](https://docs.greatexpectations.io/docs/terms/data_context/)\n",
    "\n",
    "**NOTE**: If generating the DataContext object generates *invalid store configuration* warnings, you probably forgot to provision the storage for the tutorial. Ensure these are properly provisioned before continuing with the rest of the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 1. Initialize GE and S3 objects\n",
    "context = ge.data_context.DataContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading tutorial data to Blob Storage\n",
    "\n",
    "To emulate a *normal* setting, you will now upload the tutorial data found in this repository to the container you provisioned using terraform. In order to do so, the PATH_TUTORIAL_DATA constant is set to the local path to the tutorial data. Next, a `BlobServiceClient` object is initialized using the connection string that was generated before, in order to connect to our storage account.\n",
    "\n",
    "With this client initialized, the data can then be uploaded to the data container as specified in the `testing_config` file.\n",
    "\n",
    "The data being uploaded contains information about taxi trips in New York City. More information about it can be found [here](https://registry.opendata.aws/nyc-tlc-trip-records-pds/)\n",
    "\n",
    "After running the code cell below, you can enter the Azure portal and check the container to see if the data now resides on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Start up client for container\n",
    "blob_service_client = BlobServiceClient.from_connection_string(conn_str=connection_string)\n",
    "\n",
    "# -- Path constant\n",
    "PATH_TUTORIAL_DATA = \"./data/\"\n",
    "\n",
    "# -- Upload tutorial data\n",
    "for dataset in os.listdir(PATH_TUTORIAL_DATA):\n",
    "    # -- Set path to file\n",
    "    path_file = os.path.abspath(PATH_TUTORIAL_DATA+dataset)\n",
    "    path_blob = \"data/\"+dataset\n",
    "\n",
    "    # -- Create blob_client\n",
    "    blob_client = blob_service_client.get_blob_client(container=test_config.data_container_name, blob=path_blob)\n",
    "\n",
    "    with open(path_file, \"rb\") as data:\n",
    "        blob_client.upload_blob(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data\n",
    "\n",
    "The next step is to load an example dataset that can be used to create expectations for. As previously mentioned, the function `load_csv_from_container` was already developed for this tutorial and was imported as `load_data`. \n",
    "\n",
    "Having uploaded the tutorial data to a container in the previous cell, we can now download it and transform it into a pandas DataFrame with the cell below (which is obviously redundant, but implemented for the sake of this tutorial). To know which file to load, we first get a list of objects residing on S3 using `get_file_keys_from_s3` and then pick the oldest one from this list (objects are sorted).\n",
    "\n",
    "The code of the `load_csv_from_container` function is shown below (note that it can be found in the `supporting_functions.py` file, which is also where this notebook imports it from).\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def load_csv_from_container(\n",
    "    blob_service_client: BlobServiceClient, container_name: str, path_csv: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Function that downloads a CSV from a container in an Azure storage account and\n",
    "    returns it as a pandas DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    blob_service_client : BlobServiceClient\n",
    "        BlobServiceClient for the storage account to target. Must be authenticated and\n",
    "        allowed to access and interact with containers\n",
    "    container_name : str\n",
    "        Name of the container to get a list of blobs from\n",
    "    path_csv : str\n",
    "        Path to the CSV file in the container (can be obtained by calling \n",
    "        get_file_keys_from_container)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The downloaded CSV file as a pandas DataFrame in memory\n",
    "    \"\"\"\n",
    "\n",
    "    # -- 1. Initiate blob client and download blob\n",
    "    blob_client = blob_service_client.get_blob_client(\n",
    "        container=container_name, blob=path_csv\n",
    "    )\n",
    "    downloaded_blob = blob_client.download_blob()\n",
    "\n",
    "    # -- 2. Read incoming stream as pandas DataFrame\n",
    "    df = pd.read_csv(StringIO(downloaded_blob.content_as_text()))\n",
    "\n",
    "    return df\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Pay special attention to what inputs this function needs and how it knows what file to load, since this will be used in the Azure Function as well. As you can see in the function source code, it will need an initialized `BlobServiceClient`, a container name and a path to a CSV object in order to load data.\n",
    "\n",
    "Apart from the dataset, Great Expectations needs some additional parameters for operations down the line. These are an identifier for the batch being run and a name for the data asset. These can be the same, as long as they can be used to identify which dataset is being evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Get list of blobs in data container, use the first\n",
    "list_files = get_file_keys_from_container(\n",
    "    blob_service_client=blob_service_client,\n",
    "    container_name=test_config.data_container_name,\n",
    ")\n",
    "asset_path = list_files[0]\n",
    "asset_name = asset_path.split(\"/\")[-1]\n",
    "\n",
    "# -- Load dataset\n",
    "df_batch = load_data(\n",
    "    blob_service_client=blob_service_client,\n",
    "    container_name=test_config.data_container_name,\n",
    "    path_csv=asset_path,\n",
    ")\n",
    "batch_identifier = \"tutorial_batch_dataset\"\n",
    "\n",
    "\n",
    "# -- Show top rows of the dataset to get an idea of its contents\n",
    "df_batch.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a batch request, expectation suite and validator\n",
    "Now that the data has been loaded, this batch will be passed to a RuntimeBatchRequest below in order to start building an expectation suite. An expectation suite is Great Expectations jargon for a collection of expectations (or tests) that you want to run your data against. \n",
    "\n",
    "Great Expectations requires data to be passed as a request when you want to use the context object to generate things such as expectation suites. The RuntimeBatchRequest is used here, because we are loading data (with our own logic) at runtime and want to pass that to subsequent objects.\n",
    "\n",
    "More information on runtime batch requests can be found [here](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/how_to_configure_a_runtimedataconnector/).\n",
    "\n",
    "Using the RuntimeBatchRequest, two things are done next:\n",
    "1. Generating an expectation suite: this will serve as a collection of tests you will run for future datasets\n",
    "2. Generate a validator: this object will use the batch dataset loaded previously to start running expectations and storing these in the suite\n",
    "\n",
    "As soon as the suite and validator are initiated, you can start writing expectations in the next cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 3. Generate batch request at runtime using loaded tile\n",
    "batch_request = RuntimeBatchRequest(\n",
    "    datasource_name=\"runtime_data\",\n",
    "    data_connector_name=\"runtime_data_connector\",\n",
    "    data_asset_name=asset_name,\n",
    "    runtime_parameters={\"batch_data\": df_batch},\n",
    "    batch_identifiers={\"batch_identifier\": batch_identifier,},\n",
    ")\n",
    "\n",
    "# -- 4. Generate expectation suite, start validator\n",
    "suite = context.create_expectation_suite(\n",
    "    test_config.expectations_suite_name,\n",
    "    overwrite_existing=True,  \n",
    ")\n",
    "\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=test_config.expectations_suite_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Expectations\n",
    "Using the validator object, expectations can be formulated below. Since Great Expectations comes with many expectations out of the box, [this page](https://greatexpectations.io/expectations) is generally a good place to start browsing through these. \n",
    "\n",
    "Predefined expectations can be used by calling them using the validator object and passing the required arguments. For example, to run an expectation on the number of rows in a dataframe, the following snippet can be used:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# Get number of rows of current batch\n",
    "row_count = df_batch.shape[0]\n",
    "\n",
    "# Make expectation where the maximum deviation from the batch number of rows is 1%\n",
    "max_delta = 0.01\n",
    "validator.expect_table_row_count_to_be_between(\n",
    "    min_value=row_count * (1-max_delta), max_value=row_count * (1+max_delta))\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Expectations can be run both on the level of a table, e.g. evaluating the number of rows or columns, and on the level of a column, e.g. evaluating the minimum and maximum within a column. The expected values for such tests are set when generating the expectations suite using the `validator` object in the cells below and will be used for future validations.\n",
    "\n",
    "Alternatively, expectations can also be set using dynamic evaluation parameters, which is just an expensive set of words for test values that you determine at runtime. This can be useful if you for example want to compare your current dataset with the data of last month and use values in your expectations based on last month's data. An example of how to configure these dynamic evaluation parameters is shown below. More information about them can be found [here](https://docs.greatexpectations.io/docs/reference/evaluation_parameters/)\n",
    "\n",
    "Apart from existing expectations, you can also develop expectations yourself. If you want to do so, more information can be found about that [here](https://docs.greatexpectations.io/docs/guides/expectations/creating_custom_expectations/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Table level expectations\n",
    "\n",
    "# -- Constant parameters\n",
    "ROW_COUNT_DELTA = .05\n",
    "\n",
    "# -- 1. Expect future datasets to contain the same columns as current batch DataFrame\n",
    "expected_columns = df_batch.columns\n",
    "validator.expect_table_columns_to_match_set(\n",
    "    column_set=expected_columns, exact_match=True\n",
    ")\n",
    "\n",
    "# -- 2. Expect row count to be in between range, based on set delta and number of rows of batch dataset\n",
    "row_count = df_batch.shape[0]\n",
    "validator.expect_table_row_count_to_be_between(\n",
    "    min_value=row_count * (1-ROW_COUNT_DELTA), \n",
    "    max_value=row_count * (1+ROW_COUNT_DELTA),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Column level expectations\n",
    "\n",
    "# -- 1. Values are never null\n",
    "for column in expected_columns:\n",
    "    validator.expect_column_values_to_not_be_null(column)\n",
    "\n",
    "# -- 2. Date columns are parseable\n",
    "date_columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "for column in date_columns:\n",
    "    validator.expect_column_values_to_be_dateutil_parseable(column)\n",
    "\n",
    "# -- 3. Check dtypes, assuming dtypes of the batch dataset are correct (this is\n",
    "#       something you might rather want to hard-code for real products)\n",
    "dict_dtypes = {}\n",
    "for column, dtype in zip(df_batch.dtypes.index, df_batch.dtypes):\n",
    "    dict_dtypes[column] = str(dtype)\n",
    "\n",
    "for column, dtype in dict_dtypes.items():\n",
    "    validator.expect_column_values_to_be_of_type(column, dtype)\n",
    "\n",
    "# -- 4. Expect values of specific columns to be between lower- and upper bounds\n",
    "dict_bounds = {\"VendorID\":[1,2],\n",
    "              \"payment_type\":[1,4]\n",
    "              }\n",
    "\n",
    "for column, (lower_bound, upper_bound) in dict_bounds.items():\n",
    "    # NOTE: for large datasets, this expectation is really slow. Using seperate tests\n",
    "    # for what the minimum should be and the maximum should be is a lot faster, while\n",
    "    # achieving the same test-wise\n",
    "    validator.expect_column_values_to_be_between(column, lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Expectation using dynamic evaluation parameters. Instead of passing values directly \n",
    "#    here, a dictionary is passed with the name of the dynamic parameters along with a mock\n",
    "#    value for the current validation. At runtime, this value will need to be provided \n",
    "#    when running the validations\n",
    "\n",
    "test_column = \"passenger_count\"\n",
    "max_passenger_count = df_batch[test_column].max()\n",
    "\n",
    "validator.expect_column_max_to_be_between(\n",
    "    column=test_column,\n",
    "    min_value={\n",
    "        \"$PARAMETER\": \"min_max_passenger_count\",\n",
    "        \"$PARAMETER.min_max_passenger_count\": max_passenger_count\n",
    "        },\n",
    "    max_value={\n",
    "        \"$PARAMETER\": \"max_max_passenger_count\",\n",
    "        \"$PARAMETER.max_max_passenger_count\": max_passenger_count+2\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalizing expectation suite and creating checkpoint\n",
    "After running all the expectations in the cells above, the cell below can be executed to save this set of expectations as a suite and couple it with a checkpoint. \n",
    "\n",
    "A checkpoint is an object which can be called by validation logic to run new data batches against, coupling an expectation_suite with parameters to a name. In addition, actions can be coupled to this checkpoint, such as automatically updating the Data Docs website or sending a message on Slack. \n",
    "\n",
    "This checkpoint can be used in other scripts (lambda_function.py) by passing a new batch of data (as RuntimeBatchRequest) along with the checkpoint name to the `run_checkpoint` method of an initialized DataContext (which is the same as the `context` object initialized in this notebook). Such a call would look like:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "results = context.run_checkpoint(\n",
    "    checkpoint_name=\"CHECKPOINT_NAME\",\n",
    "    validations=[{\"batch_request\": batch_request}],\n",
    "    )\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "When the code below is ran, Great Expectations automatically saves the expectation suite and checkpoint to S3 via the validator and context objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 6. Save suite\n",
    "validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "\n",
    "# -- 7.1 Create Simple checkpoint with automatic data docs updates\n",
    "checkpoint_config = {\n",
    "    \"name\": test_config.checkpoint_name,\n",
    "    \"config_version\": 3,\n",
    "    \"class_name\": \"SimpleCheckpoint\",\n",
    "    \"expectation_suite_name\": test_config.expectations_suite_name,\n",
    "    \"run_name_template\": test_config.run_name_template,\n",
    "}\n",
    "\n",
    "# -- 7.2 Add to context object\n",
    "context.add_checkpoint(**checkpoint_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating the Data Docs website\n",
    "Next, the command below can be ran to build the Data Docs website on a static website hosted on a container on Azure, which provides an interactive user interface in which you can browse through expectation suites and check validation results. More on Data Docs can be found [here](https://docs.greatexpectations.io/docs/reference/data_docs/)\n",
    "\n",
    "If you use a SimpleCheckpoint, as is the case in this tutorial, the website will automatically be updated each time validations are run. If not, you have to either manually or programmatically update the Data Docs website but calling `context.build_data_docs()`\n",
    "\n",
    "After initializing the Data Docs website, you should be able to see the expectation suite we just generated and inspect its expectations. In the following steps, we will start running validations, which well then also appear on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_site_output = context.build_data_docs()\n",
    "print_ge_site_link(ge_site_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developing logic for the Azure Function\n",
    "\n",
    "As previously stated, Grater Expectations implements data testing through deploying testing logic on an Azure Function that can be called over new data. Normally you would have to configure this Function yourself to be able to load data at runtime and run expectations. For this tutorial, the Function code has already been completed with data loading logic and in order for it to run, it expects to receive the path to the data in the container in the parameters of a POST request, so it knows what to load and what to run expectations over at runtime. The parameter that is expected is structured as `{\"path_file\":<path_to_CSV_in_container>}`\n",
    "\n",
    "\n",
    "You can find all the code for the Function in `function/grater-expectations/function.py`. To get a better understanding of the function and its steps, it is worthwhile to open this file and walk through the steps and the code. Most of the classes and functions that it uses are stored in `supporting_functions.py`, so it is also worthwhile to have a look there.\n",
    "\n",
    "In essence, the Function is rather straightforward in its steps:\n",
    "1. Initialize objects, load configuration files\n",
    "2. Load data from container to pandas using the `path_file` parameters that was attached to the POST request at runtime\n",
    "3. Convert the data to a RuntimeBatchRequest\n",
    "4. Set dynamic evaluation parameters\n",
    "5. Run validations\n",
    "6. Evaluate results\n",
    "\n",
    "As the Function does not require any tweaks, if you understand its contents and functioning, you can proceed to the next step.\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Docker image and deployment on Azure ACR\n",
    "Because there are size constraints when it comes to using Python packages (on AWS) and to ensure that whatever runs locally, also properly runs in the Cloud, Grater Expectations uses Docker containers to deploy logic.\n",
    "\n",
    "To help you setting this up, `initialize_project.py` automatically generated all the boilerplate code you need to create a Docker image and load it to ACR. Said logic can be found in:\n",
    "- **Dockerfile**: this file contains the required steps to build a new Docker image to deploy on Azure\n",
    "- **build_image_store_on_acr.sh**: bash script containing all steps to create a new Docker image using the Dockerfile and load it to ACR, provided you have the Docker Engine and Azure CLI installed and your user credentials (AWS) can be accessed\n",
    "\n",
    "Since the Azure Function is ready to be Dockerized and deployed at this stage, you can do so by calling this `build_image_store_on_acr.sh` bash script in the terminal from the directory of the tutorial, as shown below. **Make sure that Docker Engine is running before you run the bash script!**\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "# Set AWS credentials in the current terminal\n",
    "export AWS_ACCESS_KEY_ID=<enter_aws_access_key_here>\n",
    "export AWS_SECRET_ACCESS_KEY=<enter_aws_secret_access_key_here>\n",
    "\n",
    "# Go to the directory of the tutorial (if the terminal is not already opened there)\n",
    "cd tutorial\n",
    "\n",
    "# Run bash script from tutorial directory\n",
    "sh build_image_store_on_ecr.sh\n",
    "```\n",
    "**NOTE**:\n",
    "1. For Windows users, the build_image_store_on_acr.sh will not work when called from CMD or Powershell. Instead, use Git Bash (which is automatically installed on your machine when you install Git) to call the bash script. Before doing so, make sure that you export your **Azure access credentials** in the terminal, so it can interact with Azure.\n",
    "\n",
    "<br>\n",
    "\n",
    "When `build_image_store_on_acr.sh` is called, the script will build a new Docker image for Python 3.8 using a publicly available base image from Azure, install all dependencies within it based on `requirements.txt` and copy required code- and configuration files onto the image (`supporting_function.py`, `functions/` directory, `project_config.yml` and `great_expectations/great_expectations.yml`). Next, it will create a new repository on Azure ACR (if needed) and upload the Docker image to it. The output in the terminal should look as follows:\n",
    "\n",
    "![Bash output of deployment](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/bash_output_deployment_Azure.png)\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying the Azure Function\n",
    "\n",
    "Now that the Function logic is available as Docker image on ACR, the next step is to deploy an Azure Function that uses this Docker image.\n",
    "\n",
    "This can be done by using the Terraform configurations that were automatically generated when you ran `initialize_project.py` for the tutorial. In the *terraform/function* subdirectory of the tutorial, you will find the configurations you need to spin up the Function.\n",
    "\n",
    "Similarly as with the storage, open up a terminal in this directory (or open the directory in the terminal you already have opened) and run the `terraform init` and `terraform plan` commands, typing *yes* when prompted by Terraform. For completeness, see the snippet below.\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "# Set credentials (if not already set in the current terminal)\n",
    "export ARM_CLIENT_ID=\"<enter_client_id_here>\"\n",
    "export ARM_CLIENT_SECRET=\"<enter_client_secret_here>\"\n",
    "export ARM_SUBSCRIPTION_ID=\"<enter_subscription_id_here>\"\n",
    "export ARM_TENANT_ID=\"<enter_tenant_id_here>\"\n",
    "\n",
    "# Go to correct Terraform directory\n",
    "cd terraform/lambda\n",
    "\n",
    "# Initialize Terraform\n",
    "terraform init\n",
    "\n",
    "# Generate deployment plan, enter yes at the prompt if the plan is correct\n",
    "terraform apply\n",
    "```\n",
    "<br>\n",
    "\n",
    "If successfull, you will see the following outputs.\n",
    "\n",
    "\n",
    "![Terraform Lambda output](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/tutorial_tf_azure_function.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "You can click the link below to check the Azure Function in the Azure console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Web%2Fsites/kind/functionapp\"\n",
    "generate_link_in_notebook(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating new batches of data\n",
    "\n",
    "Now that the validation Function is ready to be used, the next step is to have it validate new batches of taxi trip data.  \n",
    "\n",
    "To do so, we re-use the list of prefixes to datasets in `list_files` that was previously generated. As you might recall, we used the first dataset in this list as batch dataset to configure expectations for. Now we can use the other datasets to run the expectation suite over using the Lambda function.\n",
    "\n",
    "To do so, we take the remaining prefixes in the list and generate payloads from them to serve to the Lambda. As previously stated, the Lambda function expects to receive `{\"path_file\":<path_to_dataset_on_containe>}` in the parameters of the POST request to know which dataset to load.\n",
    "\n",
    "Therefore, a list of parameters is generated below and stored in `list_params`. This list is subsequently looped over, making a POST request for each element in the list.\n",
    "\n",
    "In order to make the POST requests, you must retrieve the endpoint of the Azure Function you deployed. This can be done by entering the portal, looking up the Azure Function, selecting the Functions overview under the Functions header and selecting `grater-expectations` (as shown below). When you enter the subsequent window, you can retrieve the endpoint by clicking the `Get Function Url` button. Paste it in the `FUNCTION_ENDPOINT` variable below.\n",
    "\n",
    "![azure_functions_endpoint](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/azure_functions_function_endpoint.png)\n",
    "\n",
    "**Note**: *When you have just deployed your Function, it can take some time for it to start up and to show the function in the Functions pane. If there is not function visible yet, wait a bit longer*\n",
    "\n",
    "After calling the Function, you can check the Data Docs website for the results of running the expectation suite on the other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Get list of files to validate (drop all but the first, since we used that before)\n",
    "list_files_to_validate = list_files[1:]\n",
    "\n",
    "# -- Set list of params to push in POST requests\n",
    "list_params = []\n",
    "for path_file in list_files_to_validate:\n",
    "    params = {\"path_file\":path_file}\n",
    "    list_params.append(params)\n",
    "\n",
    "# -- Make POST requests for each of the datasets to validate\n",
    "#    NOTE: The endpoint of the function must be pulled from the Azure portal as soon as the function has been deployed\n",
    "FUNCTION_ENDPOINT = \"THIS_MUST_BE_SET\"\n",
    "list_responses = []\n",
    "for params in list_params:\n",
    "    response = requests.post(FUNCTION_ENDPOINT, params=params)\n",
    "    list_responses.append(response)\n",
    "\n",
    "# -- Print GE website link for easy access\n",
    "print_ge_site_link(ge_site_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap up of tutorial and clean-up of Azure services\n",
    "\n",
    "If you are able to see the validation runs on the Data Docs website, you have successfully completed the tutorial! You are now equipped and ready to start using Grater Expectations on your own Azure projects. To do so, you can generate a new configuration for a project in `testing_config.yml`, ensure that you fill in parameters for each of the configurations and then call `grater create project --name <project_name>`. A new directory for your project will then be bootstrapped.\n",
    "\n",
    "Alternatively, if you want to run a project on AWS (or the tutorial for that matter), you can run `grater create config --provider AWS` to create a config file for AWS. Be careful not to overwrite your existing config file though!\n",
    "\n",
    "Before doing so, you should **clean up the services** that were provisioned for running this tutorial, to prevent unnecessary costs from accruing on your Azure bill.\n",
    "\n",
    "The easiest way to do so is to open up your terminal again, browse to the *terraform/storage* directory and call `terraform destroy`. Since the resource group for all resources that were provisioned sits in these Terraform configurations, destroying it will ensure all child services are destroyed as well.\n",
    "\n",
    "<br>\n",
    "\n",
    "![Terraform Lambda destroy](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/tutorial_tf_storage_destroy.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "![Terraform Lambda destroy yes](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/tutorial_tf_storage_destroy_yes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of tutorial\n",
    "And that concludes this tutorial! If you were running this notebook through a Jupyter server, you can stop it from running by calling the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"jupyter notebook stop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c89e88f1ac255c8816e2c1f4efb25e90f69d64517a11691a0b94aad36d0a26a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
