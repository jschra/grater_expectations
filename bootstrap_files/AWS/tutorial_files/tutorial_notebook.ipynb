{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grater Expectations Tutorial\n",
    "\n",
    "![Grater Expectations](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/grater_expectations_background_small.png)\n",
    "\n",
    "### Introduction\n",
    "Welcome to Grater Expectations tutorial! This notebook will help you run through a full example of using Grater Expectations. To repeat on what is already mentioned in the README, note that you need the following to run all the components of the tutorial:\n",
    "\n",
    "-  AWS account with [programmatic access keys](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html)\n",
    "- [Docker Engine](https://docs.docker.com/engine/): to create new images to run on AWS Lambda and push them to ECR\n",
    "- [AWS CLI](https://aws.amazon.com/cli/): to login to AWS, create an ECR repository and push docker images to ECR\n",
    "- Python 3.8: It is recommended to use conda ([Miniconda](https://docs.conda.io/en/latest/miniconda.html)) for easy environment creation and management\n",
    "- [Terraform](https://www.terraform.io/): to spin up S3 buckets for GE artifacts and the Data Docs website and a Lambda function for testing\n",
    "- IDE (e.g. VS Code, optional): for easier development (not necessarily for notebooks, but definitely for Python files)\n",
    "\n",
    "If you have these installed, then you are ready to continue with the tutorial!\n",
    "\n",
    "<hr>\n",
    "\n",
    "In order to validate your data, Great Expectations is a package that offers a battery-included set of logic to get up-and-running fast. Fully figuring out how Great Expectations works and applying it to your project, however, can be somewhat involved. This is what Grater Expectations and this tutorial help you with!\n",
    "\n",
    "Grater Expectations makes a few choices for you and offers scripts, configurations and notebooks to get you started. The choices that were made are:\n",
    "\n",
    "- Great Expectations output will be stored on S3\n",
    "- The rendered Data Docs site will be stored on S3\n",
    "- You will write your own data loading logic to read data into memory as a pandas DataFrame\n",
    "- You will write your own set of expectations to test the quality of this data\n",
    "- The validation logic will be deployed as Docker container via AWS Lambda\n",
    "\n",
    "To set you up for the above, you already entered configurations in `testing_config.yml` for the tutorial, which will be used throughout the code to generate AWS services and access them.\n",
    "\n",
    "<hr>\n",
    "\n",
    "To get an idea of how Grater Expectations can help you to develop and deploy logic that you can use to test data, a simplified workflow is shown below. \n",
    "\n",
    "![Workflow](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/high_level_workflow.png)\n",
    "\n",
    "The overall idea is to **configure** data testing by using Great Expectations, so that a preset selection of tests can be run over new data at **runtime**.\n",
    "\n",
    "In order to **configure** Great Expectations, an example dataset representative of future data to be tested is loaded. Using this dataset, multiple tests (*expectations*) are defined and bundled into a set of tests (*expectation suite*). In order to call this expectation suite at runtime, it is connected to a checkpoint. This checkpoint can then be called at runtime to test new data against, the logic of which is developed a Python script and built into a Docker image. This image is deployed as an AWS Lambda.\n",
    "\n",
    "At **runtime**, an event containing information on which new data to load and test can then be sent to the deployed Lambda to invoke it. The Lambda will then load and validate the new data, using the checkpoint and expectation suite previously developed. The results will then be published on a so-called Data Docs website, which end users can then inspect.\n",
    "\n",
    "In the rest of this tutorial, each of the steps to set up services, configure Great Expectations and validate new data will be detailed.\n",
    "\n",
    "### Table of contents\n",
    "This notebook will guide you through each of the steps to get you testing your data as soon as possible. The steps are:\n",
    "\n",
    "1. [Setting up a virtual environment](#setting-up-a-virtual-environment)\n",
    "2. [Provisioning S3 buckets](#provisioning-s3-buckets)\n",
    "3. [Imports and configurations](#imports-and-configurations)\n",
    "4. [Initialization of objects](#initialization-of-objects)\n",
    "5. [Uploading tutorial data to S3](#uploading-tutorial-data-to-s3)\n",
    "6. [Loading data](#loading-data)\n",
    "7. [Generating a batch request, expectation suite and validator](#generating-a-batch-request-expectation-suite-and-validator)\n",
    "8. [Creating Expectations](#writing-expectations)\n",
    "9. [Finalizing expectation suite and creating checkpoint](#finalizing-expectation-suite-and-creating-checkpoint)\n",
    "10. [Instantiating the Data Docs website](#instantiating-the-data-docs-website)\n",
    "11. [Developing logic for the Lambda function](#developing-logic-for-the-lambda-function)\n",
    "12. [Creating a Docker image and deployment on AWS ECR](#creating-a-docker-image-and-deployment-on-aws-ecr)\n",
    "13. [Deploying the AWS Lambda function](#deploying-the-aws-lambda-function)\n",
    "14. [Validating new batches of data](#validating-new-batches-of-data)\n",
    "15. [Wrap up of tutorial and clean-up of AWS services](#wrap-up-of-tutorial-and-clean-up-of-aws-services)\n",
    "16. [End of tutorial](#end-of-tutorial)\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Setting up a virtual environment\n",
    "In order to run the logic contained within this notebook, make sure that it was started up from a virtual environment that contained all required Python dependencies. The easiest way to assure this is to first make a virtual environment for the project and then install `grater_expectations` within it.\n",
    "\n",
    "To create a new virtual environment, e.g. for python 3.8, and installing the package and its dependencies, run the following:\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "**Option 1: Pip**\n",
    "\n",
    "```bash\n",
    "# Create a virtual environment\n",
    "python -m venv env\n",
    "\n",
    "# Activate the virtual environment\n",
    "env/Scripts/Activate # Windows\n",
    "source env/bin/activate # MacOS\n",
    "\n",
    "# Install into the virtual envirpnment\n",
    "pip install grater_expectations\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Option 2: Anaconda**\n",
    "\n",
    "```bash\n",
    "# Create a conda environment\n",
    "conda create --name grater_expectations python=3.8\n",
    "\n",
    "# Activate the conda environment\n",
    "conda activate grater_expectations\n",
    "\n",
    "# Install into the virtual environment\n",
    "pip install grater_expectations\n",
    "```\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Provisioning S3 buckets\n",
    "For testing, Grater Expectations is configured to interact with S3 buckets on AWS. To be able to run all the code in this notebook the **store_bucket**, **site_bucket** and **data_bucket** as configured in the testing_config.yml must be provisioned.\n",
    "\n",
    "To do so, auto-generated Terraform files can be found in the *terraform/buckets* directory of this project. To use these configurations to generate S3 buckets in that directory, open a (Git bash) terminal, set your AWS programmatic access credentials as environment variables and run the following commands:\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "# Set credentials\n",
    "export AWS_ACCESS_KEY_ID=<enter_aws_access_key_here>\n",
    "export AWS_SECRET_ACCESS_KEY=<enter_aws_secret_access_key_here>\n",
    "export AWS_SESSION_TOKEN=<enter_session_token_here> # If using AWS SSO\n",
    "\n",
    "# Go to correct Terraform directory\n",
    "cd terraform/buckets\n",
    "\n",
    "# Initialize Terraform\n",
    "terraform init\n",
    "\n",
    "# Generate deployment plan, enter yes at the prompt if the plan is correct\n",
    "terraform apply\n",
    "```\n",
    "<br>\n",
    "\n",
    "Terraform will then provide you with a plan, which it will deploy if you enter 'yes' at the prompt as shown below.\n",
    "\n",
    "![Terraform bucket prompt](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/tutorial_tf_bucket_prompt.png)\n",
    "\n",
    "After entering yes, Terraform will deploy the buckets and will show you the following output when finished.\n",
    "\n",
    "![Terraform bucket apply](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/tutorial_tf_bucket_apply.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "You can check if the buckets were successfully created by logging in to the console and following [this link](https://s3.console.aws.amazon.com/s3/buckets)\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and configurations\n",
    "In the cell below, packages are imported and configurations are loaded from the `project_config.yml`, which was automatically created by `initialize_project.py`. This config file is based on the parameters set in `testing_config.yml` at the root of this repository\n",
    "\n",
    "Note specifically the import of load_csv_from_s3 as load_data. For this tutorial, a function was already developed to load csv files from S3 and turn these into pandas DataFrames. You can inspect the code for this function in `supporting_functions.py` or by calling `help(load_data)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports of Python and Grater Expectations packages and logic\n",
    "import great_expectations as ge\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "import boto3\n",
    "from supporting_functions import (TestingConfiguration,\n",
    "                                  get_file_keys_from_s3,\n",
    "                                  print_ge_site_link,\n",
    "                                  generate_link_in_notebook,\n",
    "                                  invoke_lambda_function)\n",
    "from supporting_functions import load_csv_from_s3 as load_data\n",
    "import json\n",
    "import os\n",
    "\n",
    "# -- Load parameters from configuration file\n",
    "test_config = TestingConfiguration(\"project_config.yml\")\n",
    "test_config.load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of objects\n",
    "Next, objects required to interact with AWS from Python and a Great Expectations DataContext are initialized.\n",
    "\n",
    "By loading the GE configuration file (`great_expectations.yml` found in the great_expectations subdirectory of this project. The DataContext class defaults to this location and file when called), the `DataContext` object stores important parameters for you to interact with GE and automatically interact with S3 buckets for storing output, pulling checkpoints and generating Data Docs sites. For more information on data contexts, check out [the documentation](https://docs.greatexpectations.io/docs/terms/data_context/)\n",
    "\n",
    "**NOTE**: If generating the DataContext object generates *invalid store configuration* warnings, you probably forgot to provision the S3 buckets for the tutorial. Ensure these are properly provisioned before continuing with the rest of the tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 1. Initialize GE and S3 objects\n",
    "s3_client = boto3.client(\"s3\")\n",
    "bucket = boto3.resource(\"s3\").Bucket(test_config.data_bucket)\n",
    "context = ge.data_context.DataContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading tutorial data to S3\n",
    "\n",
    "To emulate a *normal* setting, you will now upload the tutorial data found in this repository to the S3 data bucket you provisioned using terraform. In order to do so, the PATH_TUTORIAL_DATA constant is set to the local path to the tutorial data. Next, the initialized bucket client (which is set to the data bucket) is used to upload the data to your bucket.\n",
    "\n",
    "The data being uploaded contains information about taxi trips in New York City. More information about it can be found [here](https://registry.opendata.aws/nyc-tlc-trip-records-pds/)\n",
    "\n",
    "After running the code cell below, you can enter the AWS terminal and check the S3 bucket to see if the data now resides on it. The cell automatically provides a link to the bucket that you can click."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Path constant\n",
    "PATH_TUTORIAL_DATA = test_config.prefix_data\n",
    "\n",
    "# -- Upload logic\n",
    "for dataset in os.listdir(PATH_TUTORIAL_DATA):\n",
    "    # -- Set path to file\n",
    "    path_file = os.path.abspath(PATH_TUTORIAL_DATA+dataset)\n",
    "\n",
    "    # -- Upload to S3\n",
    "    bucket.meta.client.upload_file(\n",
    "        Filename=path_file, \n",
    "        Bucket=test_config.data_bucket, \n",
    "        Key=PATH_TUTORIAL_DATA+dataset\n",
    "    )\n",
    "\n",
    "# -- Provide link to S3 bucket\n",
    "url = f\"https://s3.console.aws.amazon.com/s3/buckets/{bucket.name}?region={test_config.region}&tab=objects\"\n",
    "generate_link_in_notebook(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data\n",
    "\n",
    "The next step is to load an example dataset that can be used to create expectations for. As previously mentioned, the function `load_csv_from_s3` was already developed for this tutorial and was imported as `load_data`. \n",
    "\n",
    "Having uploaded the tutorial data to S3 in the previous cell, we can now download it and transform it into a pandas DataFrame with the cell below (which is obviously redundant, but implemented for the sake of this tutorial). To know which file to load, we first get a list of objects residing on S3 using `get_file_keys_from_s3` and then pick the oldest one from this list (objects are sorted).\n",
    "\n",
    "The code of the `load_csv_from_s3` function is shown below.\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def load_csv_from_s3(\n",
    "    s3_bucket_client: boto3.resource(\"s3\").Bucket, prefix: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Function to loads a csv from S3 into a pandas DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s3_bucket_client : boto3.resource\n",
    "        Instantiated s3 bucket client using boto3. Note that it should already\n",
    "        be pointing to the bucket from which you want to load objects\n",
    "    prefix : str\n",
    "        Prefix to csv object on S3\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The loaded csv object as pandas DataFrame\n",
    "    \"\"\"\n",
    "    s3_object = s3_bucket_client.Object(prefix).get()\n",
    "    df = pd.read_csv(s3_object[\"Body\"])\n",
    "\n",
    "    return df\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Pay special attention to what inputs this function needs and how it knows what file to load, since this will be used in the Lambda function as well. As you can see in the function source code, it will need an initialized bucket client and a prefix to an object in order to load data.\n",
    "\n",
    "Apart from the dataset, Great Expectations needs some additional parameters for operations down the line. These are an identifier for the batch being run and a name for the data asset. These can be the same, as long as they can be used to identify which dataset is being evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Get object keys from S3, sort, pick oldest\n",
    "list_objects = get_file_keys_from_s3(s3_client, test_config.data_bucket, PATH_TUTORIAL_DATA)\n",
    "list_objects.sort()\n",
    "asset_name = list_objects[0]\n",
    "\n",
    "# -- Load dataset\n",
    "df_batch = load_data(bucket, asset_name)\n",
    "batch_identifier = \"tutorial_batch_dataset\"\n",
    "\n",
    "# -- Show top rows of the dataset to get an idea of its contents\n",
    "df_batch.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a batch request, expectation suite and validator\n",
    "Now that the data has been loaded, this batch will be passed to a RuntimeBatchRequest below in order to start building an expectation suite. An expectation suite is Great Expectations jargon for a collection of expectations (or tests) that you want to run your data against. \n",
    "\n",
    "Great Expectations requires data to be passed as a request when you want to use the context object to generate things such as expectation suites. The RuntimeBatchRequest is used here, because we are loading data (with our own logic) at runtime and want to pass that to subsequent objects.\n",
    "\n",
    "More information on runtime batch requests can be found [here](https://docs.greatexpectations.io/docs/guides/connecting_to_your_data/how_to_configure_a_runtimedataconnector/).\n",
    "\n",
    "Using the RuntimeBatchRequest, two things are done next:\n",
    "1. Generating an expectation suite: this will serve as a collection of tests you will run for future datasets\n",
    "2. Generate a validator: this object will use the batch dataset loaded previously to start running expectations and storing these in the suite\n",
    "\n",
    "As soon as the suite and validator are initiated, you can start writing expectations in the next cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 3. Generate batch request at runtime using loaded tile\n",
    "batch_request = RuntimeBatchRequest(\n",
    "    datasource_name=\"runtime_data\",\n",
    "    data_connector_name=\"runtime_data_connector\",\n",
    "    data_asset_name=asset_name,\n",
    "    runtime_parameters={\"batch_data\": df_batch},\n",
    "    batch_identifiers={\"batch_identifier\": batch_identifier,},\n",
    ")\n",
    "\n",
    "# -- 4. Generate expectation suite, start validator\n",
    "suite = context.create_expectation_suite(\n",
    "    test_config.expectations_suite_name,\n",
    "    overwrite_existing=True,  \n",
    ")\n",
    "\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=test_config.expectations_suite_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Expectations\n",
    "Using the validator object, expectations can be formulated below. Since Great Expectations comes with many expectations out of the box, [this page](https://greatexpectations.io/expectations) is generally a good place to start browsing through these. \n",
    "\n",
    "Predefined expectations can be used by calling them using the validator object and passing the required arguments. For example, to run an expectation on the number of rows in a dataframe, the following snippet can be used:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# Get number of rows of current batch\n",
    "row_count = df_batch.shape[0]\n",
    "\n",
    "# Make expectation where the maximum deviation from the batch number of rows is 1%\n",
    "max_delta = 0.01\n",
    "validator.expect_table_row_count_to_be_between(\n",
    "    min_value=row_count * (1-max_delta), max_value=row_count * (1+max_delta))\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Expectations can be run both on the level of a table, e.g. evaluating the number of rows or columns, and on the level of a column, e.g. evaluating the minimum and maximum within a column. The expected values for such tests are set when generating the expectations suite using the `validator` object in the cells below and will be used for future validations.\n",
    "\n",
    "Alternatively, expectations can also be set using dynamic evaluation parameters, which is just an expensive set of words for test values that you determine at runtime. This can be useful if you for example want to compare your current dataset with the data of last month and use values in your expectations based on last month's data. An example of how to configure these dynamic evaluation parameters is shown below. More information about them can be found [here](https://docs.greatexpectations.io/docs/reference/evaluation_parameters/)\n",
    "\n",
    "Apart from existing expectations, you can also develop expectations yourself. If you want to do so, more information can be found about that [here](https://docs.greatexpectations.io/docs/guides/expectations/creating_custom_expectations/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Table level expectations\n",
    "\n",
    "# -- Constant parameters\n",
    "ROW_COUNT_DELTA = .05\n",
    "\n",
    "# -- 1. Expect future datasets to contain the same columns as current batch DataFrame\n",
    "expected_columns = df_batch.columns\n",
    "validator.expect_table_columns_to_match_set(\n",
    "    column_set=expected_columns, exact_match=True\n",
    ")\n",
    "\n",
    "# -- 2. Expect row count to be in between range, based on set delta and number of rows of batch dataset\n",
    "row_count = df_batch.shape[0]\n",
    "validator.expect_table_row_count_to_be_between(\n",
    "    min_value=row_count * (1-ROW_COUNT_DELTA), \n",
    "    max_value=row_count * (1+ROW_COUNT_DELTA),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Column level expectations\n",
    "\n",
    "# -- 1. Values are never null\n",
    "for column in expected_columns:\n",
    "    validator.expect_column_values_to_not_be_null(column)\n",
    "\n",
    "# -- 2. Date columns are parseable\n",
    "date_columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    "for column in date_columns:\n",
    "    validator.expect_column_values_to_be_dateutil_parseable(column)\n",
    "\n",
    "# -- 3. Check dtypes, assuming dtypes of the batch dataset are correct (this is\n",
    "#       something you might rather want to hard-code for real products)\n",
    "dict_dtypes = {}\n",
    "for column, dtype in zip(df_batch.dtypes.index, df_batch.dtypes):\n",
    "    dict_dtypes[column] = str(dtype)\n",
    "\n",
    "for column, dtype in dict_dtypes.items():\n",
    "    validator.expect_column_values_to_be_of_type(column, dtype)\n",
    "\n",
    "# -- 4. Expect values of specific columns to be between lower- and upper bounds\n",
    "dict_bounds = {\"VendorID\":[1,2],\n",
    "              \"payment_type\":[1,4]\n",
    "              }\n",
    "\n",
    "for column, (lower_bound, upper_bound) in dict_bounds.items():\n",
    "    # NOTE: for large datasets, this expectation is really slow. Using seperate tests\n",
    "    # for what the minimum should be and the maximum should be is a lot faster, while\n",
    "    # achieving the same test-wise\n",
    "    validator.expect_column_values_to_be_between(column, lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Expectation using dynamic evaluation parameters. Instead of passing values directly \n",
    "#    here, a dictionary is passed with the name of the dynamic parameters along with a mock\n",
    "#    value for the current validation. At runtime, this value will need to be provided \n",
    "#    when running the validations\n",
    "\n",
    "test_column = \"passenger_count\"\n",
    "max_passenger_count = df_batch[test_column].max()\n",
    "\n",
    "validator.expect_column_max_to_be_between(\n",
    "    column=test_column,\n",
    "    min_value={\n",
    "        \"$PARAMETER\": \"min_max_passenger_count\",\n",
    "        \"$PARAMETER.min_max_passenger_count\": max_passenger_count\n",
    "        },\n",
    "    max_value={\n",
    "        \"$PARAMETER\": \"max_max_passenger_count\",\n",
    "        \"$PARAMETER.max_max_passenger_count\": max_passenger_count+2\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalizing expectation suite and creating checkpoint\n",
    "After running all the expectations in the cells above, the cell below can be executed to save this set of expectations as a suite and couple it with a checkpoint. \n",
    "\n",
    "A checkpoint is an object which can be called by validation logic to run new data batches against, coupling an expectation_suite with parameters to a name. In addition, actions can be coupled to this checkpoint, such as automatically updating the Data Docs website or sending a message on Slack. \n",
    "\n",
    "This checkpoint can be used in other scripts (lambda_function.py) by passing a new batch of data (as RuntimeBatchRequest) along with the checkpoint name to the `run_checkpoint` method of an initialized DataContext (which is the same as the `context` object initialized in this notebook). Such a call would look like:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "results = context.run_checkpoint(\n",
    "    checkpoint_name=\"CHECKPOINT_NAME\",\n",
    "    validations=[{\"batch_request\": batch_request}],\n",
    "    )\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "When the code below is ran, Great Expectations automatically saves the expectation suite and checkpoint to S3 via the validator and context objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 6. Save suite\n",
    "validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "\n",
    "# -- 7.1 Create Simple checkpoint with automatic data docs updates\n",
    "checkpoint_config = {\n",
    "    \"name\": test_config.checkpoint_name,\n",
    "    \"config_version\": 3,\n",
    "    \"class_name\": \"SimpleCheckpoint\",\n",
    "    \"expectation_suite_name\": test_config.expectations_suite_name,\n",
    "    \"run_name_template\": test_config.run_name_template,\n",
    "}\n",
    "\n",
    "# -- 7.2 Add to context object\n",
    "context.add_checkpoint(**checkpoint_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating the Data Docs website\n",
    "Next, the command below can be ran to build the Data Docs website on S3, which provides an interactive user interface in which you can browse through expectation suites and check validation results. More on Data Docs can be found [here](https://docs.greatexpectations.io/docs/reference/data_docs/)\n",
    "\n",
    "If you use a SimpleCheckpoint, as is the case in this tutorial, the website will automatically be updated each time validaitons are run. If not, you have to either manually or programmatically update the Data Docs website but calling `context.build_data_docs()`\n",
    "\n",
    "After initializing the Data Docs website, you should be able to see the expectation suite we just generated and inspect its expectations. In the following steps, we will start running validations, which well then also appear on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_site_output = context.build_data_docs()\n",
    "print_ge_site_link(ge_site_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developing logic for the Lambda function\n",
    "\n",
    "As previously stated, Grater Expectations implements data testing through deploying testing logic on an AWS Lambda function that can be called over new data. Normally you would have to configure this Lambda yourself to be able to load data at runtime and run expectations. For this tutorial, the Lambda function code has already been completed with data loading logic and in order for it to run, it expects to receive the prefix of the data file on S3 in the event so it knows what to load and what to run expectations over from its event at runtime. The JSON that is expected is structured as `{\"object_prefix\":<prefix_to_dataset_on_s3>}`\n",
    "\n",
    "\n",
    "You can find all the code for the Lambda function in `lambda_function.py`. To get a better understanding of the function and its steps, it is worthwhile to open this file and walk through the steps and the code. Most of the classes and functions that it uses are stored in `supporting_functions.py`, so it is also worthwhile to have a look there.\n",
    "\n",
    "In essence, the function is rather straightforward in its steps:\n",
    "1. Initialize objects, load configuration files\n",
    "2. Load data from S3 to pandas using the object_prefix passed in the event at runtime\n",
    "3. Convert the data to a RuntimeBatchRequest\n",
    "4. Set dynamic evaluation parameters\n",
    "5. Run validations\n",
    "6. Evaluate results\n",
    "\n",
    "As the Lambda function does not require any tweaks, if you understand its contents and functioning, you can proceed to the next step.\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Docker image and deployment on AWS ECR\n",
    "Because there are size constraints when it comes to using Python packages on AWS Lambda (max 250MB of loaded packages through layers), Grater Expectations uses Docker images instead (for which the size constraint is 10GB).\n",
    "\n",
    "To help you setting this up, `initialize_project.py` automatically generated all the boilerplate code you need to create a Docker image and load it to ECR. Said logic can be found in:\n",
    "- **Dockerfile**: this file contains the required steps to build a new Docker image to deploy on AWS\n",
    "- **build_image_store_on_ecr.sh**: bash script containing all steps to create a new Docker image using the Dockerfile and load it to ECR, provided you have the Docker Engine and AWS CLI installed and your user credentials (AWS) can be accessed\n",
    "\n",
    "Since the Lambda function is ready to be Dockerized and deployed at this stage, you can do so by calling this `build_image_store_on_ecr.sh` bash script in the terminal from the directory of the tutorial, as shown below. **Make sure that Docker Engine is running before you run the bash script!**\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "# Set AWS credentials in the current terminal\n",
    "export AWS_ACCESS_KEY_ID=<enter_aws_access_key_here>\n",
    "export AWS_SECRET_ACCESS_KEY=<enter_aws_secret_access_key_here>\n",
    "\n",
    "# Go to the directory of the tutorial (if the terminal is not already opened there)\n",
    "cd tutorial\n",
    "\n",
    "# Run bash script from tutorial directory\n",
    "sh build_image_store_on_ecr.sh\n",
    "```\n",
    "**NOTE**:\n",
    "1. If a prompt appears after creating the ECR repository, you can close it by pressing q.\n",
    "2. For Windows users, the build_image_store_on_ecr.sh will not work when called from CMD or Powershell. Instead, use Git Bash (which is automatically installed on your machine when you install Git) to call the bash script. Before doing so, make sure that you export your credentials in the terminal, so it can interact with AWS.\n",
    "\n",
    "<br>\n",
    "\n",
    "When `build_image_store_on_ecr.sh` is called, the script will build a new Docker image for Python 3.8 using a publicly available base image from AWS, install all dependencies within it based on `requirements.txt` and copy required code- and configuration files onto the image (`supporting_function.py`, `lambda_function.py`, `project_config.yml` and `great_expectations/great_expectations.yml`). Next, it will create a new repository on AWS ECR (if needed) and upload the Docker image to it. The output in the terminal should look as follows:\n",
    "\n",
    "![Bash output of deployment](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/bash_output_deployment.png)\n",
    "\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying the AWS Lambda function\n",
    "\n",
    "Now that the Lambda logic is available as Docker image on ECR, the next step is to deploy a Lambda function that uses this Docker image.\n",
    "\n",
    "This can be done by using the Terraform configurations that were automatically when you ran `initialize_project.py` for the tutorial. In the *terraform/lambda* subdirectory of the tutorial, you will find the configurations you need to spin up the Lambda.\n",
    "\n",
    "Similarly as with the S3 buckets, open up a terminal in this directory (or open the directory in the terminal you already have opened) and run the `terraform init` and `terraform plan` commands, typing *yes* when prompted by Terraform. For completeness, see the snippet below.\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "# Set credentials (if not already set in the current terminal)\n",
    "export AWS_ACCESS_KEY_ID=<enter_aws_access_key_here>\n",
    "export AWS_SECRET_ACCESS_KEY=<enter_aws_secret_access_key_here>\n",
    "\n",
    "# Go to correct Terraform directory\n",
    "cd terraform/lambda\n",
    "\n",
    "# Initialize Terraform\n",
    "terraform init\n",
    "\n",
    "# Generate deployment plan, enter yes at the prompt if the plan is correct\n",
    "terraform apply\n",
    "```\n",
    "<br>\n",
    "\n",
    "If successfull, you will see the following outputs.\n",
    "\n",
    "![Terraform Lambda output](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/tutorial_tf_lambda.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "You can click the link below to check the Lambda function in the AWS console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://{test_config.region}.console.aws.amazon.com/lambda/home?region={test_config.region}#/functions/grater_expectations_validation_tutorial?tab=code\"\n",
    "generate_link_in_notebook(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating new batches of data\n",
    "\n",
    "Now that the validation Lambda is ready to be used, the next step is to have it validate new batches of taxi trip data.  \n",
    "\n",
    "To do so, we re-use the list of prefixes to datasets in `list_objects` that was previously generated. As you might recall, we used the first dataset in this list as batch dataset to configure expectations for. Now we can use the other datasets to run the expectation suite over using the Lambda function.\n",
    "\n",
    "To do so, we take the remaining prefixes in the list and generate payloads from them to serve to the Lambda. As previously stated, the Lambda function expects to receive `{\"object_prefix\":<prefix_to_dataset_on_s3>}` in its event to know which dataset to load. When invoking the Lambda from Python, it expects to receive this JSON encoded as bytes. \n",
    "\n",
    "Therefore, the prefixes in `list_objects_lambda` are put into JSON's and then encoded. After doing so, the `invoke_lambda_function` is called sequentially for each of the datasets, where the responses of the calls to the Lambda function are stored in `responses`.\n",
    "\n",
    "After calling the Lambda, you can check the Data Docs website for the results of running the expectation suite on the other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Initialize client\n",
    "lambda_client = boto3.client('lambda', region_name=test_config.region)\n",
    "\n",
    "# -- Get object keys from S3, sort, drop oldest dataset as this was used when setting up the expectation suite\n",
    "list_objects_lambda = list_objects[1:]\n",
    "\n",
    "# -- Generate payloads for invoking the Lambda\n",
    "list_payloads = []\n",
    "for prefix in list_objects_lambda:\n",
    "    payload = {\"object_prefix\":prefix}\n",
    "    payload_bytes = json.dumps(payload).encode(\"utf-8\")\n",
    "    list_payloads.append(payload_bytes)\n",
    "\n",
    "# -- Invoke Lambda sequentially, store responses\n",
    "responses = []\n",
    "for payload in list_payloads:\n",
    "    response = invoke_lambda_function(\n",
    "        lambda_client=lambda_client, \n",
    "        payload=payload,\n",
    "        lambda_function=\"grater_expectations_validation_tutorial\")\n",
    "    responses.append(response)\n",
    "\n",
    "# -- Print GE website link for easy access\n",
    "print_ge_site_link(ge_site_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap up of tutorial and clean-up of AWS services\n",
    "\n",
    "If you are able to see the validation runs on the Data Docs website, you have successfully completed the tutorial! You are now equipped and ready to start using Grater Expectations on your own projects. To do so, you can generate a new configuration for a project in `testing_config.yml`, ensure that you fill in parameters for each of the configurations and then call `python initialize_project.py -p <project_name>`. A new directory for your project will then be bootstrapped.\n",
    "\n",
    "Before doing so, you should **clean up the services** that were provisioned for running this tutorial, to prevent unnecessary costs from accruing on your AWS bill.\n",
    "\n",
    "To do so, open up your terminal again, browse to the *terraform/buckets* directory and, thereafter, the *terraform/lambda* directory and call `terraform destroy`. An example for the Lambda configurations is shown below.\n",
    "\n",
    "<br>\n",
    "\n",
    "![Terraform Lambda destroy](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/tutorial_tf_lambda_destroy.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "![Terraform Lambda destroy yes](https://raw.githubusercontent.com/jschra/grater_expectations/main/docs/images/tutorial_tf_lambda_destroy_yes.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "After deprovisioning the S3 buckets and the Lambda function, the last step is to remove the Docker image from ECR. This can be done by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Initialize ECR object to remove repo and image\n",
    "ecr = boto3.client(\"ecr\", region_name= test_config.region)\n",
    "ecr.delete_repository(\n",
    "    registryId=test_config.account_id,\n",
    "    repositoryName=test_config.docker_image_name,\n",
    "    force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of tutorial\n",
    "And that concludes this tutorial! If you were running this notebook through a Jupyter server, you can stop it from running by calling the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"jupyter notebook stop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c89e88f1ac255c8816e2c1f4efb25e90f69d64517a11691a0b94aad36d0a26a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
